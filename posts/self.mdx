---
title: "*__self__*"
authors: [
  {
    name: "noreplydev"
  }, 
]
category: ["-"]
id: "*__self__*"
date: "2025-08-04"
---

# the problem is the output type, not the output

*__self__* is a virtual machine. Another one? well, yes, but with native AI instructions.
Think of V8 or the JVM, but designed from the ground up with AI instructions as part of the 
instruction set, that lets you do things like: 

> NOTE: every code snippet in the post is real functional code

```swift
import ai

let actions = ai.do("
infer the value of 'hello world!' in chinese and
create a file called demo.txt and put the content
on it.
")

fn exec_action(item) {
    println("executing action: ", item)
    item.exec()
}
actions.map(exec_action)
```

i dont think it's required to mention, but that code generates a file called demo.txt
with the content "hello world!" on it but, in chinese. how? it uses the *__self__* stdlib as available functions
to execute. the output of the execution will be: 

```
executing action: Action(ai.infer)
executing action: Action(fs.write_file)
```

the potential of this it's big since it not executes the function, if you look closely at the
code, it returns a vector of __Action__ structs and then you're the one who decides to execute that __Action__ or not. 
based on rules like if the operation it's destructive.

<br/>

but is this the real potential of a virtual machine with native AI integration at machine level? well that's the real
question. the AI opens a new world of solutions for semantic/contextual/non-discretizable problems. 

> NOTE: in this post, the word "AI" will be used indistinguishably with Large Language Models, as one implementation of AI. This distinction is made because future branches of AI could lead to new usage patterns.

### AI impact on deterministic software programming
One thing that has been on the table is the lack of determinism of an AI output. how the stochastic generated output affects the usage of AI 
on deterministic-required environments. And, it's true but if we think about it, that lack of determinism it's perfect, because we dont need AI
to operate on deterministic environments, we need AI to resolve semantical, contextual and non-discretizable problems. 

<br/>

we've been able to process pdf's for the past 25 or 30 years, but now with AI we can infer if the pdf is talking about "pink cats". How we could infer that without AI? pdf raw 
reading? n-grams generation from text? dictionary keywords matching for each topic? tags extraction?... basically, we based our "deterministic execution" 
on euristics. the pdf example it's very clear because it's an static problem but, what about a program execution? isn't it the same thing?

<br/>

let's go back to what a program was. turing used to define a "turing machine" like a sliced tape processor, where each tape slice means something. the whole tape it's called program. let's take that mental model and imagine
that our tape to say hello world (a.k.a our program), is something like: 

```
// two tape slices
["hello world"]-[PRINT]
```

historically this tape has had slices of different types like LOAD or JUMP to load variables or navigate through the tape respectively but, let's pick
another tape slice type like: __INFER__. what is this type? technically it could be anything we want it to be,
but the real potent thing is use them as AI instructions like __PRINT__ but (and here is the key) with their undeterministic behaviour. 
PRINT will always __PRINT__ what you told, but __INFER__? __INFER__ should allow you to generate an inference of a value based
on some input. 

<br/>

for example: __INFER__ "a random color" -> "red"

<br/>

but, a moment, in this world, the __INFER__ undeterministic output shines, it could infer values based on runtime context or on non-discretizable problems. 
Then why is a normal tend think that the AI cannot be used as a program native instruction? well, normally it's said that the problem is the output but the 
thing is that __the problem is the output type, not the output__.

if we were speaking with random people, the problem would be if we say "house" or "home" or if we say "house" and "casa" (spanish word for "house")? in that
example the output is the word, and the type is the language. with AI solutions we tend to try to enforce "house" or "home" response but the real impact is, 
can we know/enforce/cast if the AI is answering in english or spanish, metaphorically. in other words, we need to know which type outputs the AI.

<br/>


### *__self__*
*__self__*, implements a type system where each AI instruction returns a typed response, which if it fails, *__self__* itself fallbacks to a nothing type or to the enforced
type. with this approach, *__self__* can have control flow based on AI instructions returned values. since this values are generated at runtime (but they could not with a caching system) we
could create dynamic control flow, based on semantics or context, like: 

```
import ai

let input = "' or 1=1--"
if ai.infer("does <arg> input looks like an sql injection?", input) {
  println("warning!")
} else {
  println("secure!")
}
```

imagine setting this code snippet in front of a database query processor. or we could have things like a file search system based on a search topic: 

```
import fs 

let search_topic = "cats"
let files = fs.read_dir("./files")

fn iter_files(file) {
  let file_path = path.join("./files", file)
  let content = fs.read_file(file_path)
  if ai.infer("<arg> content talks about " + search_topic + "?", content) {
    println("new found at: " + file_path)
  }
}

files.map(iter_files)
```

at this point it could appear the question, what distinguishes *__self__* from a library on top of any other virtual machine. 

### *__self__* as backend
right now, *__self__* is the virtual machine without no more, but, how can we write code for *__self__* then? ego. ego is a programming language built for
*__self__* programming, but it's not required to use *__self__*. ego is a frontend for *__self__* but a compiler from a webui to *__self__* bytecode or a compiler from
python to *__self__* could be made. *__self__* is the backend, the infrastructure for the programs with native AI integration. Currently *__self__* uses available state
of the art AI models from very sound providers like OpenAI or MistralAI, allowing different providers and even defining your own custom providers.

<br/>

the *__self__* virtual machine is built in rust, so we could compile to have *__self__* on the browser, on the phone, on the desktop, even in esp32 devices. imagine esp32 with
native AI integration for contextual control flow.

<br/>

all of this is thanks to *__self__* being its own virtual machine, would be difficult to achieve all this being a simple javascript library. *__self__* is built from
zero to have 0 axioms about what should or what shouldn't do a virtual machine with native AI integration in the instruction set.

<br/>
<br/>



